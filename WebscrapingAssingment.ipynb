{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63af44fe-0598-4aca-a609-4c591ca70d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "\n",
    "#Web scraping is the automated process of extracting data from websites.\n",
    "\n",
    "\n",
    "#It is used to efficiently collect large amounts of web data for analysis or other purposes, automating what would otherwise be a time-consuming manual task.\n",
    "\n",
    "### Three Areas Where Web Scraping is Used-\n",
    "#1. **Market Research**: Collecting competitor pricing and product details.\n",
    "#2. **Real Estate**: Gathering property listings and market trends.\n",
    "#3. **Academic Research**: Collecting data from publications and online sources for studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a5171f-1e47-4654-a61a-05b6eb302348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ What are the different methods used for Web Scraping?\n",
    "\n",
    "#1. **Manual Copy-Pasting**: Manually copying data from websites; impractical for large-scale scraping.\n",
    "\n",
    "#2. **HTTP Requests**: Using libraries like `requests` to fetch raw HTML content from websites.\n",
    "\n",
    "#3. **HTML Parsing**: Using libraries like BeautifulSoup to parse and extract data from HTML.\n",
    "\n",
    "#4. **Web Scraping Frameworks**: Using frameworks like Scrapy to build and run web scraping spiders.\n",
    "\n",
    "#5. **Browser Automation Tools**: Using tools like Selenium or Puppeteer to interact with and scrape dynamic web content.\n",
    "\n",
    "#6. **API Integration**: Accessing structured data from websites via provided APIs.\n",
    "\n",
    "#7. **Headless Browsers**: Using headless browsers like Headless Chrome to scrape web pages without a GUI.\n",
    "\n",
    "#8. **Regular Expressions**: Using regex to match and extract specific patterns in HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27893beb-43c1-4cd7-bd7e-a26bc59a29fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2635202044.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    ```\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#@ What is Beautiful Soup? Why is it used? \n",
    "\n",
    "\n",
    "#Beautiful Soup is a Python library for parsing HTML and XML documents.\n",
    "\n",
    "#It is used for web scraping because it simplifies the extraction, navigation, and modification of data from web pages, handling poorly formatted HTML easily.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://example.com')\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(soup.title.text)  # Extracts the title of the web page\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e28affd0-143f-4355-9ad8-3061059fa9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Why is flask used in this Web Scraping project?\n",
    "\n",
    "#Flask is used in a web scraping project for these reasons:\n",
    "\n",
    "#1. **Web Interface**: Provides a user-friendly web interface to input URLs and parameters.\n",
    "#2. **Displaying Results**: Displays scraped data directly in the browser.\n",
    "#3. **API Creation**: Creates API endpoints to trigger scraping and return data in structured formats like JSON.\n",
    "#4. **Task Management**: Manages multiple scraping tasks initiated by different users.\n",
    "#5. **Integration**: Integrates well with other Python libraries like Beautiful Soup and `requests`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b292c956-607d-4ee8-b6f8-c3a7b56c117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "#Amazon EC2: Runs the web scraping scripts and Flask application.\n",
    "#Amazon S3: Stores large amounts of scraped data.\n",
    "#Amazon RDS: Stores structured data in a managed relational database.\n",
    "#AWS Lambda: Runs code in response to events for lightweight tasks.\n",
    "#Amazon CloudWatch: Monitors and logs the application's performance.\n",
    "#AWS IAM: Manages secure access to AWS resources.\n",
    "#AWS API Gateway: Creates and manages APIs for interacting with scraping tasks and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccdc93-8eec-4541-8f88-ce35c30e940d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
