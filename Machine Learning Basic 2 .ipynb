{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d4ce28-a2e0-4d13-bfd1-0408ac659038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Define overfitting and underfitting in machine learning.-\n",
    "\n",
    "## Overfitting:\n",
    "\n",
    "#Definition:  As a result, the model performs well on the training set but fails to generalize to new, unseen data.\n",
    "#Consequences: Overfit models tend to have poor performance on new data because they essentially memorize the training set instead of learning the underlying patterns.\n",
    "#Mitigation:\n",
    "#Regularization: Introduce regularization techniques such as L1 or L2 regularization to penalize overly complex models.\n",
    "\n",
    "## Underfitting:\n",
    "\n",
    "#Definition: Underfitting occurs when a model is too simple to capture the underlying patterns in the training data. \n",
    "#Consequences: Underfit models have a lack of expressive power and struggle to capture the true relationships in the data. They often result in inaccurate and unreliable predictions.\n",
    "#Mitigation:\n",
    "#Increase model complexity: If the model is too simple, consider using a more complex model or increasing the number of features to better capture the underlying patterns.\n",
    "#Feature engineering: Introduce more relevant features or transform existing features to provide the model with more information.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb38a3d3-872f-4aff-804a-250cad94724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Reduse Overfitting\n",
    "#Regularization: Penalize complex models.\n",
    "#Cross-Validation: Evaluate performance on different data subsets.\n",
    "#Simplify Model: Reduce complexity by limiting features or model architecture.\n",
    "#Early Stopping: Halt training when performance on validation data declines.\n",
    "#Feature Selection: Choose relevant features and discard irrelevant ones\n",
    "#Data Augmentation: Increase training dataset size with variations.\n",
    "#Ensemble Methods: Combine multiple models for more robust predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f14522-1751-43e3-8bf4-510fd7f6a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Overfitting -\n",
    "##Model is too simple, leading to poor performance.\n",
    "\n",
    "#Scenarios:\n",
    "\n",
    "#Insufficient model complexity\n",
    "#Limited features\n",
    "#Low training time\n",
    "#Over-regularization\n",
    "#Too few training examples\n",
    "#Ignoring interaction terms\n",
    "#Ignoring domain knowledge\n",
    "#Inadequate hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3063fdb-46fa-470d-b382-0aed300507b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Explain the bias-variance tradeoff in machine learning. What is the relationship between bias andnvariance, and how do they affect model performance?\n",
    "\n",
    "#@Bias-Variance Tradeoff:\n",
    "\n",
    "#Definition: The bias-variance tradeoff is a key concept in machine learning that involves finding the right balance between bias and variance to achieve optimal model performance.\n",
    "\n",
    "#@ Bias:\n",
    "\n",
    "#Definition: Bias is the error introduced by approximating a real-world problem with a simplified model. High bias can lead to underfitting, where the model fails to capture the underlying patterns in the data.\n",
    "#Effect on Performance: High bias results in poor model performance on both the training and new data.\n",
    "\n",
    "#@Variance:\n",
    "\n",
    "#Definition: Variance is the model's sensitivity to fluctuations in the training data. High variance can lead to overfitting, where the model captures noise in the training data rather than the actual patterns.\n",
    "#Effect on Performance: High variance results in good performance on the training data but poor generalization to new, unseen data.\n",
    "\n",
    "#@Relationship:\n",
    "#There is a tradeoff between bias and variance. Increasing model complexity typically reduces bias but increases variance, and vice versa. The goal is to find the optimal balance that minimizes both bias and variance, leading to good generalization.\n",
    "\n",
    "#@Impact on Model Performance:\n",
    "\n",
    "#A model with high bias and low variance may be too simple and fail to capture the complexities of the data.\n",
    "#A model with low bias and high variance may overfit the training data and generalize poorly to new data.\n",
    "#The ideal model has a balanced tradeoff, achieving low bias and low variance for optimal performance.\n",
    "\n",
    "#@Strategies:\n",
    "\n",
    "#Adjusting model complexity, using regularization, and optimizing hyperparameters are strategies to manage the bias-variance tradeoff.\n",
    "#Techniques such as cross-validation help assess model performance and guide the selection of an appropriate tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b77b9ad1-57e5-4863-99b4-711e68d07531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Detecting Overfitting and Underfitting Methods:\n",
    "\n",
    "#Learning Curves:\n",
    "\n",
    "#Overfitting: Train and validation curves diverge.\n",
    "#Underfitting: Low performance on both train and validation data.\n",
    "\n",
    "#Cross-Validation:\n",
    "\n",
    "#Overfitting: Model performs well on training, poorly on validation.\n",
    "#Underfitting: Poor performance on both sets.\n",
    "\n",
    "#Validation Set Performance:\n",
    "\n",
    "#Overfitting: High training, low validation performance.\n",
    "#Underfitting: Low performance on both sets.\n",
    "\n",
    "#Model Evaluation Metrics:\n",
    "\n",
    "#Overfitting: Low training error, high validation error.\n",
    "#Underfitting: High error on both sets.\n",
    "\n",
    "#Residual Analysis:\n",
    "\n",
    "#Overfitting: Residuals show a pattern, not random.\n",
    "#Underfitting: Residuals have a consistent pattern.\n",
    "\n",
    "#Complexity Metrics:\n",
    "\n",
    "#Overfitting: High model complexity.\n",
    "#Underfitting: Low model complexity.\n",
    "\n",
    "#Regularization Parameter:\n",
    "\n",
    "#Overfitting: Increase regularization if applicable.\n",
    "#Underfitting: Reduce regularization if applicable.\n",
    "\n",
    "#Feature Importance:\n",
    "\n",
    "#Overfitting: Model relies heavily on noise features.\n",
    "#Underfitting: Ignores important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1136df8d-6e00-459d-9ec4-4884c39dd37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Bias Vs Variance-\n",
    "#Bias: Underfitting, too simple, poor on both.\n",
    "#Variance: Overfitting, captures noise, good on training, poor on new.\n",
    "#Examples: Linear regression (high bias), High-degree polynomial regression (high variance).\n",
    "#Remedy: Adjust model complexity to find the right balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c13da-bbc8-4831-9fc5-b33f8c8bdd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Regularization in Machine Learning:\n",
    "\n",
    "#Definition: Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the cost function, discouraging overly complex models.\n",
    "\n",
    "#Techniques\n",
    "-\n",
    "#Regularization: Prevents overfitting by adding penalties for complexity.\n",
    "#L1 and L2: Control the size and sparsity of coefficients.\n",
    "#Dropout: Randomly deactivates neurons to prevent reliance.\n",
    "#Early Stopping: Halts training to avoid overfitting.\n",
    "#Data Augmentation: Introduces diversity in the training data.\n",
    "#Parameter Tying: Constrains certain model parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
