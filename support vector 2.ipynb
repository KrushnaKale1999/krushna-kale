{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b6a904-61f2-4499-a870-d8851e4cf776",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (433825599.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 38\u001b[0;36m\u001b[0m\n\u001b[0;31m    ```\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Q1. Relationship Between Polynomial Functions and Kernel Functions\n",
    "#In machine learning, polynomial functions can be used as kernel functions in Support Vector Machines (SVMs). A polynomial kernel allows the SVM to create nonlinear decision boundaries by implicitly mapping input features into a higher-dimensional space. The polynomial kernel function is defined as:\n",
    "#where \\( d \\) is the degree of the polynomial, and \\( c \\) is a constant term. This kernel enables the SVM to capture more complex relationships between the features.\n",
    "\n",
    "### Q2. Implementing an SVM with a Polynomial Kernel Using Scikit-learn\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Only use two classes for binary classification\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM classifier with polynomial kernel\n",
    "clf = SVC(kernel='poly', degree=3, C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "```\n",
    "\n",
    "### Q3. Effect of Increasing Epsilon on the Number of Support Vectors in SVR\n",
    "#In Support Vector Regression (SVR), increasing the value of epsilon (\\( \\epsilon \\)) widens the margin of tolerance within which no penalty is given for errors. This generally leads to a decrease in the number of support vectors, as more data points fall within the epsilon margin.\n",
    "\n",
    "### Q4. Effects of Parameters on SVR Performance\n",
    "#- **Kernel Function:** Determines the type of decision boundary (linear, polynomial, RBF). Choose based on the problem's nature:\n",
    " # - **Linear:** Simple, linear relationships.\n",
    "  #- **Polynomial:** Complex, polynomial relationships.\n",
    "  #- **RBF:** Complex, non-linear relationships.\n",
    "\n",
    "#- **C Parameter:** Controls the trade-off between smooth decision boundary and classification of training points:\n",
    "  #- **High \\( C \\):** Less regularization, more support vectors, lower bias.\n",
    "  #- **Low \\( C \\):** More regularization, fewer support vectors, higher bias.\n",
    "\n",
    "#- **Epsilon (\\( \\epsilon \\)):** Controls the margin of tolerance for errors:\n",
    "  #- **High \\( \\epsilon \\):** Wider margin, fewer support vectors.\n",
    "  #- **Low \\( \\epsilon \\):** Narrow margin, more support vectors.\n",
    "\n",
    "#- **Gamma (RBF Kernel):** Defines how far the influence of a single training example reaches:\n",
    "  #- **High gamma:** Closer reach, more complex model, risk of overfitting.\n",
    "  #- **Low gamma:** Wider reach, smoother decision boundary, risk of underfitting.\n",
    "\n",
    "### Q5. Assignment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc = SVC(kernel='linear', random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Tune the hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'poly', 'rbf'], 'degree': [2, 3, 4], 'gamma': ['scale', 'auto']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X, y)\n",
    "\n",
    "# Save the trained classifier\n",
    "joblib.dump(best_svc, 'svc_model.pkl')\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "```\n",
    "\n",
    "This code outlines the entire process from loading the dataset to training an SVM classifier, tuning hyperparameters, evaluating performance, and saving the trained model. Adjust parameters and techniques as needed based on specific requirements and dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af23b9c-ea41-44b8-bcf7-c793fb873df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
