{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e309065b-f38d-44a8-ad6d-8dc29bbb3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Explain the difference between simple linear regression and multiple linear regression. Provide anexample of each.\n",
    "\n",
    "\n",
    "#**Simple Linear Regression:**\n",
    "#- Models the relationship between one independent variable and one dependent variable.\n",
    "#- Example: Predicting weight based on height.\n",
    "\n",
    "#**Multiple Linear Regression:**\n",
    "#- Models the relationship between multiple independent variables and one dependent variable.\n",
    "#- Example: Predicting house price based on size, number of bedrooms, and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e82cda2-53d0-4177-a686-0e715fff60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Assumptions of Linear Regression:\n",
    "\n",
    "#1. **Linearity:** The relationship between the independent and dependent variables is linear.\n",
    "#2. **Independence:** The residuals (errors) are independent of each other.\n",
    "#3. **Homoscedasticity:** The variance of the residuals is constant across all levels of the independent variables.\n",
    "#4. **Normality:** The residuals are normally distributed.\n",
    "#5. **No Multicollinearity:** The independent variables are not highly correlated with each other.\n",
    "\n",
    "#Checking Assumptions:\n",
    "\n",
    "#1. **Linearity:** Visualize scatterplots of independent variables against the dependent variable.\n",
    "#2. **Independence:** Examine autocorrelation plots of residuals or Durbin-Watson statistic.\n",
    "#3. **Homoscedasticity:** Plot residuals against predicted values or independent variables.\n",
    "#4. **Normality:** Check histogram or Q-Q plot of residuals.\n",
    "#5. **No Multicollinearity:** Calculate correlation matrix or variance inflation factors (VIF) for independent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870f4ea8-6b18-4e6b-b2ee-44295da50eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@# How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario\n",
    "\n",
    "#- **Slope (\\( m \\)):** Represents the rate of change in the dependent variable for a one-unit change in the independent variable.\n",
    "#- **Intercept (\\( b \\)):** Represents the value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "#For example, in a model predicting electricity bill based on the number of appliances:\n",
    "#- **Slope:** If it's 0.05, each additional appliance increases the bill by $0.05.\n",
    "#- **Intercept:** If it's 20, the bill is $20 when no appliances are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b2b89c9-16b1-4c62-abab-57b7a83e7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "#Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models by iteratively adjusting model parameters.\n",
    "\n",
    "#- **Concept:** It works by calculating the gradient of the cost function with respect to each model parameter.\n",
    "#The algorithm then moves in the direction opposite to the gradient to find the minimum of the cost function.\n",
    "\n",
    "#- **Usage:** Gradient descent is used in training machine learning models, such as linear regression, logistic regression, and neural networks,\n",
    "#to find the optimal parameters that minimize the difference between predicted and actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef73e0ee-4033-4ca5-9c11-39f94a44d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multicollinearity in multiple linear regression occurs when two or more independent variables are highly correlated with each other. This can cause issues in the regression analysis, such as unstable coefficient estimates and difficulty in interpreting the effects of individual predictors.\n",
    "\n",
    "#**Detection:**\n",
    "#1. **Correlation Matrix:** Calculate the correlation coefficients between pairs of independent variables. High correlations (close to 1 or -1) indicate multicollinearity.\n",
    "#2. **Variance Inflation Factor (VIF):** Calculate the VIF for each independent variable. VIF values greater than 5 or 10 suggest multicollinearity.\n",
    "\n",
    "#**Addressing:**\n",
    "#1. **Feature Selection:** Remove one of the highly correlated variables.\n",
    "#2. **Principal Component Analysis (PCA):** Transform the correlated variables into a new set of uncorrelated variables.\n",
    "#3. **Regularization Techniques:** Use techniques like Ridge Regression or Lasso Regression that penalize large coefficients and mitigate multicollinearity effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf302f78-2986-4abb-8e63-2efe4154b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advantages of Polynomial Regression:\n",
    "\n",
    "#Flexibility: Can capture nonlinear relationships between variables.\n",
    "#Higher Order Fitting: Can accommodate complex patterns in data.\n",
    "#Improved Fit: Often provides better fit to nonlinear data compared to linear regression.\n",
    "\n",
    "#Disadvantages of Polynomial Regression:\n",
    "\n",
    "#Overfitting: High-degree polynomials can lead to overfitting, especially with limited data.\n",
    "#Interpretability: Interpretation of coefficients becomes more complex with higher-degree polynomials.\n",
    "#Extrapolation Risk: Extrapolating beyond the range of observed data can be unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb90b9-53f8-448b-8d70-eadbccf4ef64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
